
import streamlit as st
import streamlit.components.v1 as html
import numpy as np
import pandas as pd
import requests
import folium
from folium.plugins import MiniMap
from streamlit_folium import st_folium

# torch
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import gluonnlp as nlp
import numpy as np
from tqdm import tqdm, tqdm_notebook

# kobert
from kobert.utils import get_tokenizer
from kobert.pytorch_kobert import get_pytorch_kobert_model

#transformers
from transformers import AdamW
from transformers.optimization import get_cosine_schedule_with_warmup

# í˜ì´ì§€ì˜ ê¸°ë³¸ ì„¤ì • êµ¬ì„±
st.set_page_config(
 layout="wide",
 page_title='ì˜¤ëŠ˜ ì´ê±° ë¨¹ì–´')

# ë°°ê²½í™”ë©´ ì„¤ì •
def add_bg_from_url():
    st.markdown(
         f"""
         <style>
         .stApp {{
             background-image: url("https://i.pinimg.com/564x/30/ed/e7/30ede74766f91c06e51f920b40a4cafb.jpg");
             background-attachment: fixed;
             background-size: cover
         }}
         </style>
         """,
         unsafe_allow_html=True
     )

add_bg_from_url()

#######################################################################################################
#### ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ####

# device = torch.device("cuda:0") #GPUì‚¬ìš©
device = torch.device("cpu")  #CPUì‚¬ìš©

bertmodel, vocab = get_pytorch_kobert_model()

tokenizer = get_tokenizer()
tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)

class BERTDataset(Dataset):
    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):
        transform = nlp.data.BERTSentenceTransform(
            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)

        self.sentences = [transform([i[sent_idx]]) for i in dataset]
        self.labels = [np.int32(i[label_idx]) for i in dataset]

    def __getitem__(self, i):
        return (self.sentences[i] + (self.labels[i], ))

    def __len__(self):
        return (len(self.labels))

max_len = 64
batch_size = 64
warmup_ratio = 0.1
num_epochs = 20
max_grad_norm = 1
log_interval = 200
learning_rate =  5e-5

class BERTClassifier(nn.Module):
    def __init__(self,
                 bert,
                 hidden_size = 768,
                 num_classes=17,
                 dr_rate=None,
                 params=None):
        super(BERTClassifier, self).__init__()
        self.bert = bert
        self.dr_rate = dr_rate
                 
        self.classifier = nn.Linear(hidden_size , num_classes)
        if dr_rate:
            self.dropout = nn.Dropout(p=dr_rate)
    
    def gen_attention_mask(self, token_ids, valid_length):

        attention_mask = torch.zeros_like(token_ids)
        for i, v in enumerate(valid_length):
            attention_mask[i][:v] = 1
        return attention_mask.float()

    def forward(self, token_ids, valid_length, segment_ids):
        attention_mask = self.gen_attention_mask(token_ids, valid_length)

        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device), return_dict=False)

        if self.dr_rate:
            out = self.dropout(pooler)
        return self.classifier(out)

model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)

no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0} ]

optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)
loss_fn = nn.CrossEntropyLoss()

def calc_accuracy(X,Y):
    max_vals, max_indices = torch.max(X, 1)
    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]
    return train_acc

def softmax(vals, idx):
    valscpu = vals.detach().cpu().squeeze(0)
    a = 0
    for i in valscpu:
        a += np.exp(i)
    return ((np.exp(valscpu[idx]))/a).item() * 100


def testModel(model, seq):
    cate = ["ê³±ì°½","êµ­ìˆ˜","ëˆì¹´ì¸ ", "ë””ì €íŠ¸","ë¼ë©˜","ë²„ê±°", "ë² ì´ì»¤ë¦¬", "ë¶„ì‹", "ìŠ¤ì‹œ", "ì•„ì‹œì•„ìŒì‹", "ì–‘ì‹", "ì „ê³¨", "ì¤‘ì‹", "ì¹˜í‚¨", "íƒ€ì½”", "í•œì‹", "í•´ì‚°ë¬¼"]
    tmp = [seq]
    transform = nlp.data.BERTSentenceTransform(tok, max_len, pad=True, pair=False)
    tokenized = transform(tmp)

    modelload.eval()
    result = modelload(torch.tensor([tokenized[0]]).to(device), [tokenized[1]], torch.tensor(tokenized[2]).to(device)) 
    idx = result.argmax().cpu().item() #ì¶œë ¥ì˜ ìµœëŒ€ê°’ì´ ë‚˜ì˜¤ê²Œí•¨
    result2 = F.softmax(result, dim=1).sort() #ê° ê°’ì— ëŒ€í•œ softmaxí•¨ìˆ˜ ì ìš©

    #return cate[idx], softmax(result,idx)
    return cate[result2[1][0][-1]],round((result2[0][0][-1]).item(), 4)*100, cate[result2[1][0][-2]],round((result2[0][0][-2]).item(), 4)*100, cate[result2[1][0][-3]],round((result2[0][0][-3]).item(), 4)*100

# ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ í•œë²ˆë§Œ ë¡œë“œí•˜ê³  ìºì‹œì— ì €ì¥í•˜ê¸°
@st.cache_resource
def cache_model(path, modelname):
    modelload = torch.load("/content/drive/MyDrive/final project/model/model6.pt", map_location=torch.device('cpu')) # cpuì‚¬ìš©ì‹œ
    # modelload = torch.load("/content/drive/MyDrive/final project/model/model6.pt") # gpuì‚¬ìš©ì‹œ
    modelload.eval()
    return modelload

modelload = cache_model('/content/drive/MyDrive/final project/model/','model6.pt')

# ì¹´ì¹´ì˜¤ api
@st.cache_resource
def elec_location(region,page_num):
    url = 'https://dapi.kakao.com/v2/local/search/keyword.json'
    params = {'query': region,'page': page_num, 'sort' : 'popularity'}
    headers = {"Authorization": "KakaoAK 6dd31dbd3f7b90aed3f5591fdde29527"}

    places = requests.get(url, params=params, headers=headers).json()['documents']

    return places

def elec_info(places):
    X = []
    Y = []
    stores = []
    road_address = []
    phone = []
    place_url = []
    ID = []
    for place in places:
        X.append(float(place['x']))
        Y.append(float(place['y']))
        stores.append(place['place_name'])
        road_address.append(place['road_address_name'])
        phone.append(place['phone'])
        place_url.append(place['place_url'])
        ID.append(place['id'])

    ar = np.array([ID,stores, X, Y, road_address, phone, place_url]).T
    df = pd.DataFrame(ar, columns = ['ID','stores', 'X', 'Y','road_address','phone','place_url'])
    return df

def keywords(location_name):
    df = None
    page_num = int(1)
    for loca in location_name:
        for page in range(1,page_num+1):
            local_name = elec_location(loca, page)
            local_elec_info = elec_info(local_name)

            if df is None:
                df = local_elec_info
            elif local_elec_info is None:
                continue
            else:
                df = pd.concat([df, local_elec_info],join='outer', ignore_index = True)
    return df

def make_map(dfs, m):
    
    minimap = MiniMap() 
    m.add_child(minimap)

    for i in range(len(dfs)):
        folium.Marker([dfs['Y'][i],dfs['X'][i]],
                      tooltip=dfs['stores'][i],
                      popup = '<iframe width="800" height="400" src="' + df['place_url'][i] + '"title="YouTube video player" frameborder="0" allow="accelerometer; autoplay;  clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>',
                      ).add_to(m)
    return m

#######################################################################################################

# st.sidebar.header('Side Menu')
tab1, tab2 = st.tabs(['search', 'map'])

# user ì…ë ¥ê°’ ì €ì¥
if 'user_input' not in st.session_state:
    st.session_state['user_input'] = ''

if 'user_location_input' not in st.session_state:
    st.session_state['user_location_input'] = ''

# with st.sidebar:
#         when = st.selectbox('ì‹ì‚¬ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?', ['ì•„ì¹¨', 'ì ì‹¬', 'ì €ë…'])
#         location = st.text_input('ì§€ê¸ˆ ê³„ì‹  ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”', key='user_location_input')

with tab1:
    st.subheader('ğŸ’­ì˜¤ëŠ˜ë„ ë¬´ì—‡ì„ ë¨¹ì„ì§€ ê³ ë¯¼í•˜ê³  ê³„ì‹ ê°€ìš”?')

    value = st.text_area('ì§€ê¸ˆ ìƒê°ë‚˜ëŠ” í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ê³  Ctrl+Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!', placeholder = 'Ex) ìœ¡ì¦™ì´ íŒ¡íŒ¡ í„°ì§€ëŠ” ê³ ì†Œí•œ ìŒì‹ì´ ë¨¹ê³ ì‹¶ì–´.', key='user_input')
    cat1,val1, cat2,val2, cat3,val3 = testModel(model ,st.session_state.user_input)

    if value:
       st.header(cat1, 'ì´ ìŒì‹ì€ ì–´ë– ì‹ ê°€ìš”?')

       st.subheader(f"{cat1}ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤. ì‹ ë¢°ë„ëŠ” {round(val1, 2)}% ì…ë‹ˆë‹¤.")
       #st.write(cat1, 'ì´(ê°€) ê°€ì¥ ì í•©í•œ ìŒì‹ì…ë‹ˆë‹¤.', 'ì‹ ë¢°ë„ëŠ”', round(val1, 2), '% ì…ë‹ˆë‹¤.')

       st.write('ì…ë ¥ë¬¸ì¥ê³¼ ê°€ì¥ ì¼ì¹˜í•˜ëŠ” ìŒì‹ TOP3 ì…ë‹ˆë‹¤.')
       st.write('ğŸ¥‡',cat1, 'ì‹ ë¢°ë„ëŠ”', round(val1, 2),'% ì…ë‹ˆë‹¤.')
       st.write('ğŸ¥ˆ',cat2, 'ì‹ ë¢°ë„ëŠ”', round(val2, 2),'% ì…ë‹ˆë‹¤.')
       st.write('ğŸ¥‰',cat3, 'ì‹ ë¢°ë„ëŠ”', round(val3, 2),'% ì…ë‹ˆë‹¤.')


with tab2:
    st.subheader('ğŸš‡ê°€ì‹œë ¤ëŠ” ì§€ì—­ì´ ì–´ë””ì¸ê°€ìš”?')
    location = st.text_input('ì§€í•˜ì² ì—­ì„ ê¸°ë°˜ìœ¼ë¡œ ìŒì‹ì ì„ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.', value = '', placeholder = 'ê·¼ì²˜ ì§€í•˜ì²  ì—­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”. ex) ê°•ë‚¨ì—­', key='user_location_input')
    user_location = st.session_state.user_location_input

    if location:
        kakao_location = [user_location + ' ' + cat1]
        try:
          df = keywords(kakao_location)
          lat = 0
          lon = 0
          for i in df['Y']:
              lat += float(i)
          for j in df['X']:
              lon += float(j)
          lat = lat/len(df['Y'])
          lon = lon/len(df['X'])
          m = folium.Map(kakao_location=[lat, lon],   # ê¸°ì¤€ì¢Œí‘œ: current_location
                        zoom_start=16)
          make_map = make_map(df, m)
          st_folium(make_map, width = 1000, height = 500, zoom=16, center = [lat, lon])
          df = df.drop(columns = ['ID', 'X', 'Y'])
          st.dataframe(df)
          st.write('ê²°ê³¼ëŠ” ì¸ê¸°ë„ìˆœìœ¼ë¡œ ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.')
        except:
          st.write('ì•„ì‰½ê²Œë„ ' + user_location + ' ê·¼ì²˜ì—ëŠ” ' + cat1 + ' ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤ã… ã… ')
